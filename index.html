
<!DOCTYPE html>

<html>

<head>
   <style>
      td, th {
        border: 0px solid black;          
        }
      img{
   padding: 5px;
}
      </style>

  <title>MeGA-CDA</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="shortcut icon" href="./static/images/cvpr/jhu_web.png" />

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.css">
  <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

<script src="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>

<style>
    body {
        font-family: 'Google Sans', sans-serif;
    }
    .top-nav {
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 0.5rem; /* Reduced padding */
        background-color: #ffffff;
    }
    .nav-center {
        display: flex;
        align-items: center;
        gap: 10px; /* Reduced gap */
    }
    .home-icon {
        font-size: 1.3rem; /* Slightly reduced size */
        color: #4a4a4a;
        text-decoration: none;
    }
    .dropdown {
        position: relative;
        display: inline-block;
    }
    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #f9f9f9;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
        z-index: 1;
        left: 50%;
        transform: translateX(-50%);
    }
    .dropdown:hover .dropdown-content {
        display: block;
    }
    .dropdown-content a {
        color: black;
        padding: 12px 16px;
        text-decoration: none;
        display: block;
        text-align: center;
    }
    .dropdown-content a:hover {
        background-color: #f1f1f1;
    }
    .publication-title {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-top: 1rem; /* Reduced top margin */
        margin-bottom: 1rem;
    }
    .publication-authors {
        font-size: 1.2rem;
        text-align: center;
        margin-bottom: 1rem;
    }
    .author-block {
        display: inline-block;
        margin-right: 10px;
    }
</style>

<body>

   <nav class="top-nav">
    <div class="nav-center">
        <a href="#" class="home-icon">üè†</a>
        <div class="dropdown">
            <span>More Research ‚ñº</span>
            <div class="dropdown-content">
                <a href="https://vibashan.github.io/possam-web/" target="_blank">PosSAM</a>
                <a href="https://kartik-3004.github.io/facexformer_web/" target="_blank">FaceXformer</a>
                <a href="https://vibashan.github.io/ovis-web/" target="_blank">Mask-Free OVIS</a>
                <a href="https://viudomain.github.io/irg-sfda-web/" target="_blank">IRG SFDA</a>
                <a href="https://viudomain.github.io/tt-sfuda-web/" target="_blank">TT SFDA</a>
            </div>
        </div>
    </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MeGA-CDA: Memory Guided Attention for Category-Aware
            Unsupervised Domain Adaptive Object Detection</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vibashan.github.io/">Vibashan VS</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=jNjvdEgAAAAJ&view_op=list_works&sortby=pubdate>Vikram Gupta</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=9dhBHuAAAAAJ&hl=en">Poojan Oza</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.vishwanathsindagi.com/">Vishwanath A. Sindagi</a><sup>1</sup>,
            </span>
              <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/">Vishal M. Patel</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Johns Hopkins University,</span>
            <span class="author-block"><sup>2</sup>Mercedes-Benz Research and Development India</span>
          </div>
                                      
          <div class="column has-text-centered">
            <a href="as">CVPR 2020</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2021/html/VS_MeGA-CDA_Memory_Guided_Attention_for_Category-Aware_Unsupervised_Domain_Adaptive_Object_CVPR_2021_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/images/cvpr/pdf.svg" alt="PDF icon" width="32" height="32">
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2103.04224"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing approaches for unsupervised domain adaptive object detection perform feature alignment via adversarial training. While these methods achieve reasonable improvements in performance, they typically perform
            category-agnostic domain alignment, thereby resulting in
            negative transfer of features. To overcome this issue, in
            this work, we attempt to incorporate category information
            into the domain adaptation process by proposing Memory
            Guided Attention for Category-Aware Domain Adaptation
            (MeGA-CDA). The proposed method consists of employing
            category-wise discriminators to ensure category-aware feature alignment for learning domain-invariant discriminative features. However, since the category information is
            not available for the target samples, we propose to generate memory-guided category-specific attention maps which
            are then used to route the features appropriately to the corresponding category discriminator. The proposed method is
            evaluated on several benchmark datasets and is shown to
            outperform existing approaches.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <h5 class="subtitle has-text-centered"></h5> 
          <center>Overview of the proposed framework</center>
            <img src="./static/images/cvpr/Archi_v2.svg" alt="" border=0 height=500 width=1500></img></
          <p>
            Source and target features are aligned through global domain adaptation and categoryaware domain adaptation. 
            Category-aware alignment is achieved by employing K category-specific discriminators. Since target labels are
            unavailable, the features to these discriminators are routed using memory-guided category-specific attention maps. Note that the arrows
            indicate the flow of source and target images during the training process.
          </p> 
          <div class="columns is-centered">
            <!-- Level Sets. -->
            <div class="column is-half">
              <div class="content">
                <h3 class="title is-4">Memory block</h3>
                <div class="level-set has-text-justified">
                  <div class="level-set-shapes">
                    <img src="./static/images/cvpr/mem_block_v1.svg" style="width: 100%"/>
                  </div>
                  <p>
                    A memory module has two operations, namely write and
                    read. To write in to the memory, features extracted from
                    the neural network are used to update the memory elements
                    appropriately. Whereas, the memory read operation is
                    used by the features extracted from the neural network to
                    query the memory and retrieve the most similar memory
                    element (or prototypical feature).
                  </p>
                </div>
              </div>
            </div>

            <!--/ Level Sets. -->
            <div class="column is-half">
              <div class="content has-text-justified">
                <h3 class="is-4">Category-Aware Attention</h3>
                <div class="level-set-ox-shapes">
                  <img src="./static/images/cvpr/mem_attn_block_v1.svg" style="width: 100%"/>
                </div>
                <p>
                  For determining the attention at a particular
                  location, we use the feature at this location as a query to
                  retrieve relevant items from the different category-specific
                  memory networks. The retrieved items are then compared
                  with the query item and based on the similarity, we compute the category-specific attention map. Furthermore, in
                  order to improve the effectiveness of the memory module and the attention map generation process, we propose
                  a metric-learning based approach that involves learning an
                  appropriate similarity metric based on the available weaksupervision in the source domain.
                </p>
              </div>
            </div>
      
          </div>
       </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <h5 class="subtitle has-text-centered"></h5> 
          <div class="columns is-centered">
            <!-- Level Sets. -->
            <div class="column is-half">
              <div class="content">
                <div class="level-set has-text-justified">
                  <div class="level-set-shapes">
                    <img src="./static/images/cvpr/mem_attn.svg"  style="width: 100%"/>
                  </div>
                  <p>
                    Qualitative detection results. Global alignment results in
                    miss-detections. In contrast, the proposed approach reduces false-
                    positives while achieving high-quality detections.
                  </p>
                </div>
              </div>
            </div>

            <!--/ Level Sets. -->
            <div class="column is-half">
              <div class="content has-text-justified">
                <div class="level-set-ox-shapes">
                  <img src="./static/images/cvpr/detect_vis.png"  style="width: 100%"/>
                  <p>
                    Comparison of attention maps computed using cosine similarity (top-row) and learned similarity based attention
                    (bottom-row). Though cosine similarity based provides reasonable focus on category features, learned similarity obtains more
                    accurate attention.
                  </p>
                </div>
              </div>
            </div>
      
          </div>
       </div>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container content is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre><code>
   @inproceedings{vs2021mega,
     title={Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection},
     author={Vs, Vibashan and Gupta, Vikram and Oza, Poojan and Sindagi, Vishwanath A and Patel, Vishal M},
     booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
     pages={4516--4526},
     year={2021}
   }</code></pre>
  </div>
</section>

<section class="section" >
  <div class="container is-max-desktop content">
    <h5 class="title">Website Template taken from <span class="author-block">
              <a href="https://nerfies.github.io/" target="_blank">Nerfies</a></h5>

  </div>
</section>

</body>
</html>
